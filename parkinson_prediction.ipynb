{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "D_MODEL = 48\n",
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LR = 0.00034\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1106b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_weights_layer = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, rnn_outputs):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        attn_energies = self.attn_weights_layer(torch.tanh(rnn_outputs)) \n",
    "        \n",
    "        \n",
    "        attn_energies = attn_energies.squeeze(-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        alpha = F.softmax(attn_energies, dim=-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        context_vector = torch.bmm(alpha.unsqueeze(1), rnn_outputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return context_vector.squeeze(1), alpha\n",
    "\n",
    "\n",
    "class Hybrid_CNN_RNN_Model(nn.Module):\n",
    "    def __init__(self, num_features_ts, num_meta_features, num_classes,\n",
    "                 cnn_out_channels=64, rnn_hidden_size=128, rnn_layers=1,\n",
    "                 meta_hidden_dim=32, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=num_features_ts, out_channels=cnn_out_channels // 2, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(cnn_out_channels // 2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=cnn_out_channels // 2, out_channels=cnn_out_channels, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(cnn_out_channels)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=cnn_out_channels, out_channels=cnn_out_channels * 2, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(cnn_out_channels * 2)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=cnn_out_channels * 2, \n",
    "                              hidden_size=rnn_hidden_size,\n",
    "                              num_layers=rnn_layers,\n",
    "                              bidirectional=True, \n",
    "                              batch_first=True,   \n",
    "                              dropout=dropout_rate if rnn_layers > 1 else 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.attention = Attention(feature_dim=rnn_hidden_size * 2)\n",
    "        \n",
    "        \n",
    "        self.meta_fc1 = nn.Linear(num_meta_features, meta_hidden_dim * 2)\n",
    "        self.meta_fc2 = nn.Linear(meta_hidden_dim * 2, meta_hidden_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        classifier_input_dim = (rnn_hidden_size * 2) + meta_hidden_dim\n",
    "        self.fc_combine1 = nn.Linear(classifier_input_dim, classifier_input_dim // 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc_final = nn.Linear(classifier_input_dim // 2, num_classes)\n",
    "\n",
    "    def forward(self, x_ts, x_meta):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x_ts)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        x = x.permute(0, 2, 1) \n",
    "        \n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        context_vector, attn_weights = self.attention(lstm_out) \n",
    "        \n",
    "        \n",
    "        \n",
    "        meta_processed = F.relu(self.meta_fc1(x_meta))\n",
    "        meta_processed = F.relu(self.meta_fc2(meta_processed)) \n",
    "        \n",
    "        \n",
    "        \n",
    "        combined_features = torch.cat((context_vector, meta_processed), dim=1)\n",
    "        \n",
    "        \n",
    "        out = F.relu(self.fc_combine1(combined_features))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc_final(out) \n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, manifest_df, data_dir, target_encoder,\n",
    "                 meta_label_encoders=None,\n",
    "                 meta_scalers=None,\n",
    "                 max_seq_len=MAX_SEQ_LEN,\n",
    "                 is_train=True):\n",
    "        \n",
    "        self.manifest_df = manifest_df.copy()\n",
    "        self.data_dir = data_dir\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.target_encoder = target_encoder\n",
    "        \n",
    "        self.meta_cols = ['пол', 'полных лет', 'пациент off/on', 'определенный диагноз врачом (0-5)']\n",
    "        self.categorical_meta_cols = ['пол', 'пациент off/on']\n",
    "        self.numerical_meta_cols = ['полных лет', 'определенный диагноз врачом (0-5)']\n",
    "\n",
    "        if is_train:\n",
    "            self.meta_label_encoders = {col: LabelEncoder() for col in self.categorical_meta_cols}\n",
    "            self.meta_scalers = {col: col for col in self.numerical_meta_cols}\n",
    "        else:\n",
    "            if meta_label_encoders is None or meta_scalers is None:\n",
    "                raise ValueError(\"meta_label_encoders and meta_scalers must be provided when is_train=False\")\n",
    "            self.meta_label_encoders = meta_label_encoders\n",
    "            self.meta_scalers = meta_scalers\n",
    "\n",
    "        for col in self.categorical_meta_cols:\n",
    "            le = self.meta_label_encoders[col]\n",
    "            self.manifest_df[col] = self.manifest_df[col].astype(str)\n",
    "            if is_train:\n",
    "                self.manifest_df[col] = le.fit_transform(self.manifest_df[col])\n",
    "            else:\n",
    "                self.manifest_df[col] = self.manifest_df[col].apply(\n",
    "                    lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "                )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.manifest_df['Folder Path Encoded'] = self.target_encoder.transform(self.manifest_df['Folder Path'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.manifest_df.iloc[idx]\n",
    "        file_name = row['имя файла данных']\n",
    "        label = row['Folder Path Encoded']\n",
    "        \n",
    "        ts_data_path = os.path.join(self.data_dir, file_name)\n",
    "        ts_data = None \n",
    "\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            df_temp = pd.read_csv(ts_data_path, header=None, na_values=['NA', '?', '', ' '])\n",
    "            \n",
    "            \n",
    "            for col in df_temp.columns:\n",
    "                df_temp[col] = pd.to_numeric(df_temp[col], errors='coerce')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            df_temp = df_temp.fillna(0) \n",
    "\n",
    "            ts_data = df_temp.values.astype(np.float32) \n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: File not found {ts_data_path}. Returning zeros.\")\n",
    "            ts_data = np.zeros((1, 64), dtype=np.float32) \n",
    "            \n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"ERROR: File {ts_data_path} is empty. Returning zeros.\")\n",
    "            ts_data = np.zeros((1, 64), dtype=np.float32)\n",
    "        except ValueError as ve: \n",
    "            print(f\"ERROR: ValueError converting data in {ts_data_path} to numeric: {ve}. Check for non-numeric values. Returning zeros.\")\n",
    "            ts_data = np.zeros((1, 64), dtype=np.float32)\n",
    "        except Exception as e: \n",
    "            print(f\"ERROR: Could not read or process {ts_data_path}: {e}. Returning zeros.\")\n",
    "            ts_data = np.zeros((1, 64), dtype=np.float32)\n",
    "\n",
    "        \n",
    "        if ts_data is None or ts_data.ndim != 2:\n",
    "             print(f\"CRITICAL ERROR: ts_data for {file_name} is invalid after read attempt. Shape: {ts_data.shape if ts_data is not None else 'None'}. Returning zeros.\")\n",
    "             ts_data = np.zeros((1, 64), dtype=np.float32)\n",
    "\n",
    "\n",
    "        \n",
    "        if ts_data.shape[1] != 64:\n",
    "            \n",
    "            if ts_data.shape[1] > 64:\n",
    "                ts_data = ts_data[:, :64]\n",
    "            else: \n",
    "                padding_features = np.zeros((ts_data.shape[0], 64 - ts_data.shape[1]), dtype=np.float32)\n",
    "                ts_data = np.hstack((ts_data, padding_features))\n",
    "        \n",
    "        \n",
    "        current_seq_len = ts_data.shape[0]\n",
    "        if current_seq_len > self.max_seq_len:\n",
    "            ts_data = ts_data[:self.max_seq_len, :]\n",
    "        elif current_seq_len < self.max_seq_len:\n",
    "            padding = np.zeros((self.max_seq_len - current_seq_len, ts_data.shape[1]), dtype=np.float32)\n",
    "            ts_data = np.vstack((ts_data, padding))\n",
    "        \n",
    "        \n",
    "        ts_data_tensor = torch.FloatTensor(ts_data).T \n",
    "        \n",
    "        meta_features_values = row[self.meta_cols].values\n",
    "        try:\n",
    "            \n",
    "            meta_features = torch.FloatTensor(meta_features_values.astype(np.float32))\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Could not convert meta features to float for row {idx}: {meta_features_values}. Using zeros.\")\n",
    "            meta_features = torch.zeros(len(self.meta_cols), dtype=torch.float32)\n",
    "\n",
    "        return ts_data_tensor, meta_features, torch.LongTensor([label]).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fdbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs, val_loader=None):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for i, batch_data in enumerate(train_loader):\n",
    "            if batch_data is None: \n",
    "                print(f\"Skipping a problematic batch at index {i}\")\n",
    "                continue\n",
    "            ts_inputs, meta_inputs, labels = batch_data\n",
    "            ts_inputs, meta_inputs, labels = ts_inputs.to(device), meta_inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(ts_inputs, meta_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            if (i + 1) % (max(1, len(train_loader) // 5)) == 0: \n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        epoch_loss = running_loss / (len(train_loader) - sum(1 for b in train_loader if b is None)) if (len(train_loader) - sum(1 for b in train_loader if b is None)) > 0 else float('inf')\n",
    "        epoch_acc = 100 * correct_train / total_train if total_train > 0 else 0\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Training Loss: {epoch_loss:.4f}, Training Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "        if val_loader:\n",
    "            evaluate_model(model, val_loader, criterion, device, epoch, epochs)\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion, device, epoch=None, epochs=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in val_loader:\n",
    "            if batch_data is None:\n",
    "                continue\n",
    "            ts_inputs, meta_inputs, labels = batch_data\n",
    "            ts_inputs, meta_inputs, labels = ts_inputs.to(device), meta_inputs.to(device), labels.to(device)\n",
    "            outputs = model(ts_inputs, meta_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / (len(val_loader) - sum(1 for b in val_loader if b is None)) if (len(val_loader) - sum(1 for b in val_loader if b is None)) > 0 else float('inf')\n",
    "    accuracy = 100 * correct_val / total_val if total_val > 0 else 0\n",
    "    epoch_str = f\"Epoch [{epoch+1}/{epochs}] \" if epoch is not None else \"\"\n",
    "    print(f\"{epoch_str}Validation Loss: {avg_loss:.4f}, Validation Acc: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97124bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANIFEST_FILE = \"train.csv\"\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "try:\n",
    "    manifest_df = pd.read_csv(MANIFEST_FILE, encoding='utf-8')\n",
    "    print(f\"Successfully loaded '{MANIFEST_FILE}'. Shape: {manifest_df.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{MANIFEST_FILE}' not found.\")\n",
    "    user_choice = input(f\"Create dummy files for testing? (yes/no): \").lower()\n",
    "    if user_choice == 'yes':\n",
    "\n",
    "        global NUM_CLASSES\n",
    "        _dummy_df = create_dummy_files_for_testing(\n",
    "            data_dir=DATA_DIR, manifest_file=MANIFEST_FILE)\n",
    "\n",
    "        NUM_CLASSES = _dummy_df['Folder Path'].nunique()\n",
    "        manifest_df = _dummy_df\n",
    "        print(f\"Updated NUM_CLASSES to {NUM_CLASSES} based on dummy data.\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Exiting. Please provide a 'train.csv' and data files in './data/' directory.\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading '{MANIFEST_FILE}': {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Using {len(manifest_df)} samples from manifest.\")\n",
    "if 'Folder Path' not in manifest_df.columns:\n",
    "    print(\"ERROR: 'Folder Path' column missing in manifest. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "actual_num_classes = manifest_df['Folder Path'].nunique()\n",
    "if NUM_CLASSES != actual_num_classes:\n",
    "    print(\n",
    "        f\"Warning: Initial NUM_CLASSES was {NUM_CLASSES}, but found {actual_num_classes} unique 'Folder Path' values. Adjusting NUM_CLASSES.\")\n",
    "    NUM_CLASSES = actual_num_classes\n",
    "\n",
    "target_label_encoder = LabelEncoder()\n",
    "target_label_encoder.fit(manifest_df['Folder Path'])\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    manifest_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=manifest_df['Folder Path']\n",
    ")\n",
    "\n",
    "train_dataset = PatientDataset(\n",
    "    manifest_df=train_df,\n",
    "    data_dir=DATA_DIR,\n",
    "    target_encoder=target_label_encoder,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "fitted_meta_label_encoders = train_dataset.meta_label_encoders\n",
    "fitted_meta_scalers = train_dataset.meta_scalers\n",
    "\n",
    "val_dataset = PatientDataset(\n",
    "    manifest_df=val_df,\n",
    "    data_dir=DATA_DIR,\n",
    "    target_encoder=target_label_encoder,\n",
    "    meta_label_encoders=fitted_meta_label_encoders,\n",
    "    meta_scalers=fitted_meta_scalers,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "\n",
    "def collate_fn_skip_none(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=2, pin_memory=True, collate_fn=collate_fn_skip_none)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=2, pin_memory=True, collate_fn=collate_fn_skip_none)\n",
    "\n",
    "print(\n",
    "    f\"Number of training samples (approx, after potential skips): {len(train_df)}\")\n",
    "print(\n",
    "    f\"Number of validation samples (approx, after potential skips): {len(val_df)}\")\n",
    "\n",
    "num_meta_features = len(train_dataset.meta_cols)\n",
    "model = Hybrid_CNN_RNN_Model(\n",
    "    num_features_ts=64,\n",
    "    num_meta_features=num_meta_features,\n",
    "    num_classes=NUM_CLASSES\n",
    "\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(\n",
    "    f\"Model created. Number of classes: {NUM_CLASSES}. Using device: {DEVICE}\")\n",
    "print(\n",
    "    f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_model(model, train_loader, criterion, optimizer,\n",
    "            DEVICE, EPOCHS, val_loader=val_loader)\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
